{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from data_preparator import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DAC(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_classes):\n",
    "        super(DAC, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        #Inicializacao da rede\n",
    "        self.embedding = nn.Embedding(self.vocab_size+1, self.embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)#, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_classes)\n",
    "\n",
    "    def forward(self, seq, lengths, gpu = False):\n",
    "        print(\"Sequence shape: \", seq.shape)\n",
    "        print('Lengths',lengths)\n",
    "        bs = seq.size(1)\n",
    "        print(\"Batch size: \", bs)\n",
    "        self.hidden = self._init_hidden(bs, gpu)\n",
    "        \n",
    "        embeds = self.embedding(seq)\n",
    "        embeds = pack_padded_sequence(embeds, lengths) #faz o unpad da sequencia\n",
    "        \n",
    "        gru_out, self.hidden = self.gru(embeds, self.hidden) #retorna o hidden_state de todos os timesteps\n",
    "        \n",
    "        gru_out, lenghts = pad_packed_sequence(gru_out) # faz o pad da sequencia para o tamanho maximo do batch\n",
    "        \n",
    "        print('GRU output(all timesteps)', gru_out.shape)\n",
    "        print(gru_out)\n",
    "        \n",
    "        #Como é um problema de classificacao, vou usar a ultima camada hidden\n",
    "        output = self.fc(self.hidden[-1])\n",
    "        \n",
    "        return F.log_softmax(output, dim=-1)\n",
    "    \n",
    "    def _init_hidden(self, batch_size, gpu):\n",
    "        if gpu: return Variable(torch.zeros((1,batch_size,self.hidden_size)).cuda())\n",
    "        else: return Variable(torch.zeros((1,batch_size,self.hidden_size)))\n",
    "        return self.create_variable(hidden)\n",
    "\n",
    "    def create_variable(self, tensor):\n",
    "        # Do cuda() before wrapping with variable\n",
    "        if torch.cuda.is_available():\n",
    "            return Variable(tensor.cuda())\n",
    "        else:\n",
    "            return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded, labels, vocab_size, x_lengs = prepare_dataset('./conversas_mexidas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAC(\n",
      "  (embedding): Embedding(2492, 20)\n",
      "  (gru): GRU(20, 30)\n",
      "  (fc): Linear(in_features=30, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m = DAC(vocab_size, embedding_dim=20, hidden_size=30, n_classes=7)\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, encoded_dialogues, labels, x_lengs):\n",
    "        self.len = encoded_dialogues.shape[0]\n",
    "        self.x_data = encoded_dialogues\n",
    "        self.y_data = torch.tensor(labels) #one-hot encoding\n",
    "        self.x_lengs = x_lengs\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        xb = self.x_data[index]\n",
    "        yb = self.y_data[index]\n",
    "        lens = self.x_lengs[index]\n",
    "        return xb, yb, lens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_batch(x, y, lenghts):\n",
    "    lengths,indx = lenghts.sort(dim = 0, descending = True)\n",
    "    x = x[indx]\n",
    "    y = y[indx]\n",
    "    \n",
    "    return x.transpose(0,1), y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DialogueDataset(padded, labels, x_lengs)\n",
    "train_loader = DataLoader(dataset= dataset,\n",
    "                         batch_size = 2,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, l = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys, ls = sort_batch(x, y, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence shape:  torch.Size([90, 2])\n",
      "Lengths tensor([19, 13])\n",
      "Batch size:  2\n",
      "GRU output(all timesteps) torch.Size([19, 2, 30])\n",
      "tensor([[[-0.0174, -0.1684, -0.0289,  ..., -0.2198, -0.2431, -0.3154],\n",
      "         [-0.1500,  0.2103,  0.1374,  ...,  0.3475,  0.1614,  0.3104]],\n",
      "\n",
      "        [[-0.2729,  0.3935,  0.0519,  ..., -0.1029,  0.1529,  0.2242],\n",
      "         [-0.4077, -0.0337, -0.1808,  ..., -0.0506,  0.2561, -0.0474]],\n",
      "\n",
      "        [[-0.0710,  0.1958,  0.4545,  ..., -0.0450, -0.2108,  0.2041],\n",
      "         [-0.4253,  0.0620,  0.2413,  ..., -0.2340, -0.1148, -0.2630]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0684,  0.4713,  0.2873,  ...,  0.1249,  0.1252,  0.3841],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0907,  0.2221,  0.2798,  ..., -0.1970, -0.1563,  0.1902],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.3063,  0.1299,  0.1441,  ..., -0.0278, -0.0119,  0.2414],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "outp = m(xs, ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0948, -1.8105, -1.9382, -1.7673, -2.2825, -1.9622, -1.8579],\n",
       "        [-1.8108, -1.8765, -1.7892, -1.7943, -2.2329, -2.0093, -2.2191]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SÃO AS PROBABILIDADES DE CADA CLASSE \n",
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-1.7673, -1.7892], grad_fn=<MaxBackward0>), tensor([3, 2]))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(outp, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
